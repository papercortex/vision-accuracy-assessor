# AI Analysis for 28_02_2024

## Metrics

```json
{
  "title": {
    "precision": 0.5499999999999999,
    "recall": 0.3807692307692307,
    "f1Score": 0.44999999999999996
  },
  "time": {
    "precision": 0.6111111111111112,
    "recall": 0.4230769230769231,
    "f1Score": 0.5
  },
  "date": {
    "precision": 1,
    "recall": 0.6923076923076923,
    "f1Score": 0.8181818181818181
  },
  "tags": {
    "precision": 0.21428571428571427,
    "recall": 0.5,
    "f1Score": 0.3
  }
}
```

## Overall Metrics

```json
{
  "precision": 0.5938492063492063,
  "recall": 0.49903846153846154,
  "f1Score": 0.5170454545454545,
  "totalAttributes": 4
}
```

## AI Analysis

The given report provides metrics for the accuracy of the GPT-4 vision model in interpreting handwritten notes. The accuracy is described in terms of Precision, Recall, and F1 Score for various attributes - Title, Time, Date, and Tags. The "overallMetrics" aggregates the performance across these attributes.

Let's break down each of the attribute metrics:

### Title

- **Precision**: 54.99% - Suggests that when the model predicted a title, it was correct about 54.99% of the time.
- **Recall**: 38.08% - Indicates that the model identified 38.08% of all relevant titles in the dataset.
- **F1 Score**: 45% - The balance between precision and recall for titles, showing a moderate performance.

### Time

- **Precision**: 61.11% - When the model predicted times (fromTime and toTime), it was correct 61.11% of the time.
- **Recall**: 42.31% - The model identified 42.31% of all relevant times in the dataset.
- **F1 Score**: 50% - Demonstrates a better balance between precision and recall for time than for titles, but still shows room for improvement.

### Date

- **Precision**: 100% - The model perfectly predicted dates when it chose to provide a prediction.
- **Recall**: 69.23% - It correctly identified 69.23% of all relevant dates in the dataset.
- **F1 Score**: 81.82% - This is the highest F1 score among the metrics, suggesting that the model is most reliable when interpreting dates.

### Tags

- **Precision**: 21.43% - The accuracy of the model's tag predictions is low, with only 21.43% precision.
- **Recall**: 50% - The model was able to identify half of the relevant tags in the dataset.
- **F1 Score**: 30% - Reflects the poor balance between precision and recall, indicating that tag interpretation is currently unreliable.

### Overall Metrics

- **Precision**: 59.38% - On average, across all attribute predictions, the model’s precision is relatively moderate.
- **Recall**: 49.90% - The model identified just under half of all relevant attributes in the dataset, which is a moderate recall rate.
- **F1 Score**: 51.70% - Shows an overall moderate balance between precision and recall across all attributes.
- **Total Attributes**: 4 - Represents the total number of attributes the metrics were calculated for.

From this analysis, it’s clear that the model performs best at date predictions and struggles significantly with tag predictions. The title and time predictions are moderate and could also benefit from improvements.

In practical terms, if this model is being used in a production environment where the extraction and interpretation of the handwritten notes are critical, the performance especially in tags would likely need substantial improvement, and enhancements in title and time attributes might also be warranted to reach a level of reliability acceptable for operational use.

---

## Human Investigation: Kareem

Found out that some tasks where completely missing because the AI considered as sub tasks and didn't add them to the list.

### Mitigation: Short-time

I added a new feature: the user will be able to add sub tasks, check image associated. This feature should mitigate the issues :)

Implementation: I updated task schema to have subTasks: string[] and updated the prompt to instruct the AI with the updated schema.

### Long-term

We probably need to update metrics calculation to match tasks that need to be compared first instead of comparing based on the index.
